{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://192.168.2.75:7077\")\\\n",
    "            .appName(\"codecheck\")\\\n",
    "            .config(\"spark.executor.cores\",4)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df1 =  spark_session.read.json('hdfs://192.168.2.75:9000/reddit_data/1gb_data/RC_2011-08.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(subreddit='AskReddit', count=1801041)]\n"
     ]
    }
   ],
   "source": [
    "df5 = df1.select(\"author\",\"subreddit\")\\\n",
    "        .groupBy(\"subreddit\")\\\n",
    "        .count()\\\n",
    "        .orderBy([\"count\"],ascending=False)\\\n",
    "        .limit(1)\n",
    "\n",
    "print(df5.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(body='[deleted]'), Row(body=\"How do you think you're going to become responsible enough for a job? Do you believe time will take care of that?\"), Row(body=\"I can't speak for other women, but I love facial hair.  Well, that's not entirely true.  I love *beards.*  Mustaches and goatees and chinstraps aren't my thing.  \"), Row(body='Guess we know the answer.'), Row(body=\"Well this woman clearly is evil and manipulative, waiting to falsely accuse this man of abuse. So your consternation is sort of misplaced. \\n\\nBut really, r/mensrights is built to help people like throwaway2019. Literally, they exist to support people like him during crises like the one he is going through, when more official channels can be less than willing to help him out. I can't see any problem with suggesting he find help there.\"), Row(body='[something similar to this](http://i.imgur.com/ohYbN.jpg)'), Row(body=\"I never had the heart to tell him it wasn't funny.\"), Row(body='No, just make sure you have parents willing to support you into your 30s and 40s.'), Row(body=\"Oh bollocks, as if. When 7/7 happened we didn't all automatically wander to the kitchen and switch the kettle on. Stop propagating the myth that we're all Hugh Grant for fucks sake. \"), Row(body=\"Worked at a Hot n' Now (Similar to Checkers) mid 90's.  They had some stores close down in FL due to health code violations.  Instead of losing money on the product they trucked it out to other stores.  I worked in upstate SC, somewhere along the lines the trucks refrigeration quit and most of the product thawed.  Manager had it loaded in the freezer to use anyway.\")]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = df5.join(df1.select(\"subreddit\",\"body\"),df5.subreddit==df1.subreddit)\\\n",
    "        .select(\"body\").rdd\\\n",
    "\n",
    "print(rdd1.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deleted'], ['how', 'do', 'you', 'think', \"you're\", 'going', 'to', 'become', 'responsible', 'enough', 'for', 'a', 'job', 'do', 'you', 'believe', 'time', 'will', 'take', 'care', 'of', 'that'], ['i', \"can't\", 'speak', 'for', 'other', 'women', 'but', 'i', 'love', 'facial', 'hair', '', 'well', \"that's\", 'not', 'entirely', 'true', '', 'i', 'love', 'beards', '', 'mustaches', 'and', 'goatees', 'and', 'chinstraps', \"aren't\", 'my', 'thing', '', ''], ['guess', 'we', 'know', 'the', 'answer'], ['well', 'this', 'woman', 'clearly', 'is', 'evil', 'and', 'manipulative', 'waiting', 'to', 'falsely', 'accuse', 'this', 'man', 'of', 'abuse', 'so', 'your', 'consternation', 'is', 'sort', 'of', 'misplaced', 'but', 'really', 'r/mensrights', 'is', 'built', 'to', 'help', 'people', 'like', 'throwaway2019', 'literally', 'they', 'exist', 'to', 'support', 'people', 'like', 'him', 'during', 'crises', 'like', 'the', 'one', 'he', 'is', 'going', 'through', 'when', 'more', 'official', 'channels', 'can', 'be', 'less', 'than', 'willing', 'to', 'help', 'him', 'out', 'i', \"can't\", 'see', 'any', 'problem', 'with', 'suggesting', 'he', 'find', 'help', 'there'], ['something', 'similar', 'to', 'thishttp//iimgurcom/ohybnjpg'], ['i', 'never', 'had', 'the', 'heart', 'to', 'tell', 'him', 'it', \"wasn't\", 'funny'], ['no', 'just', 'make', 'sure', 'you', 'have', 'parents', 'willing', 'to', 'support', 'you', 'into', 'your', '30s', 'and', '40s'], ['oh', 'bollocks', 'as', 'if', 'when', '7/7', 'happened', 'we', \"didn't\", 'all', 'automatically', 'wander', 'to', 'the', 'kitchen', 'and', 'switch', 'the', 'kettle', 'on', 'stop', 'propagating', 'the', 'myth', 'that', \"we're\", 'all', 'hugh', 'grant', 'for', 'fucks', 'sake', ''], ['worked', 'at', 'a', 'hot', \"n'\", 'now', 'similar', 'to', 'checkers', 'mid', \"90's\", '', 'they', 'had', 'some', 'stores', 'close', 'down', 'in', 'fl', 'due', 'to', 'health', 'code', 'violations', '', 'instead', 'of', 'losing', 'money', 'on', 'the', 'product', 'they', 'trucked', 'it', 'out', 'to', 'other', 'stores', '', 'i', 'worked', 'in', 'upstate', 'sc', 'somewhere', 'along', 'the', 'lines', 'the', 'trucks', 'refrigeration', 'quit', 'and', 'most', 'of', 'the', 'product', 'thawed', '', 'manager', 'had', 'it', 'loaded', 'in', 'the', 'freezer', 'to', 'use', 'anyway']]\n"
     ]
    }
   ],
   "source": [
    "from re import sub\n",
    "\n",
    "def lower_tokenize(content):\n",
    "    clean_content = content.lower()\n",
    "    clean_content = sub(r'[^A-Za-z0-9 \\'/]+', u'', clean_content)\n",
    "    clean_content = clean_content.replace(u'\\xa0', u' ')\n",
    "    clean_content = clean_content.split(' ')\n",
    "    return clean_content\n",
    "\n",
    "tokenized_rdd1 = rdd1\\\n",
    "    .map(lambda content: lower_tokenize(content[0]))\\\n",
    "\n",
    "\n",
    "print(tokenized_rdd1.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1888915), ('and', 1228079), ('you', 822382), ('that', 708855), ('for', 465084), ('was', 442019), ('with', 357729), ('have', 348063), ('but', 342508), ('not', 315108), ('this', 312043), ('are', 280663), ('just', 257993), ('your', 254691), ('they', 253077), ('like', 236984), ('out', 205622), ('about', 198618), (\"don't\", 190521), ('all', 190133)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "word_rdd1 = tokenized_rdd1\\\n",
    "    .flatMap(lambda t: [a for a in t if len(a)>2])\\\n",
    "    .map(lambda w: (w,1))\n",
    "\n",
    "word2_rdd1 = word_rdd1.reduceByKey(add)\n",
    "\n",
    "print(word2_rdd1.takeOrdered(20, key=lambda x: -x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    stop_time = time.time()\n",
    "time_4_cores = stop_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
