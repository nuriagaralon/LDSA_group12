{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (8 cores, 16gb per machine) x 5 = 40 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.75:7077\") \\\n",
    "        .appName(\"egemen_reddit\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "        #.config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "    \n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =  spark_session.read.json('hdfs://192.168.2.75:9000/data1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+---+--------+---------+------------+-----+--------+----------+------------+---+\n",
      "|    author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded| id| link_id|parent_id|retrieved_on|score|stickied| subreddit|subreddit_id|ups|\n",
      "+----------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+---+--------+---------+------------+-----+--------+----------+------------+---+\n",
      "|      frjo|                  null|             null|A look at Vietnam...|               0| 1134365188|         null| false|     0|c13|t3_17863| t3_17863|  1473738411|    2|   false|reddit.com|        t5_6|  2|\n",
      "|   zse7zse|                  null|             null|The site states \"...|               0| 1134365725|         null| false|     0|c14|t3_17866| t3_17866|  1473738411|    1|   false|reddit.com|        t5_6|  1|\n",
      "| [deleted]|                  null|             null|Jython related to...|               0| 1134366848|         null| false|     0|c15|t3_17869| t3_17869|  1473738411|    0|   false|reddit.com|        t5_6|  0|\n",
      "| [deleted]|                  null|             null|           [deleted]|               0| 1134367660|         null| false|     0|c16|t3_17870| t3_17870|  1473738411|    1|   false|reddit.com|        t5_6|  1|\n",
      "|   rjoseph|                  null|             null|Saft is by far th...|               0| 1134367754|         null| false|     0|c17|t3_17817| t3_17817|  1473738411|    1|   false|reddit.com|        t5_6|  1|\n",
      "| [deleted]|                  null|             null|           [deleted]|               0| 1134368231|         null| false|     0|c18|t3_17710| t3_17710|  1473738411|   -6|   false|reddit.com|        t5_6| -6|\n",
      "|  cavedave|                  null|             null|How to take panor...|               0| 1134371050|         null| false|     0|c19|t3_17876| t3_17876|  1473738411|    1|   false|reddit.com|        t5_6|  1|\n",
      "|      b0se|                  null|             null|I donât know wh...|               0| 1134371664|         null| false|     0|c20|t3_17878| t3_17878|  1473738413|    1|   false|reddit.com|        t5_6|  1|\n",
      "|     damir|                  null|             null|LinkIt by Marc, a...|               0| 1134375273|         null| false|     0|c21|t3_17890| t3_17890|  1473738413|    1|   false|reddit.com|        t5_6|  1|\n",
      "|richardk74|                  null|             null|Making websites r...|               0| 1134380167|         null| false|     0|c22|t3_17901| t3_17901|  1473738413|    1|   false|reddit.com|        t5_6|  1|\n",
      "|  kn0thing|                  null|             null|On the bright sid...|               0| 1134381378|         null| false|     0|c24|t3_17830| t3_17830|  1473738413|    1|   false|reddit.com|        t5_6|  1|\n",
      "|   bugbear|                  null|             null|Like a lot of peo...|               0| 1134384119|         null| false|     0|c26|t3_17844| t3_17844|  1473738413|    8|   false|reddit.com|        t5_6|  8|\n",
      "| [deleted]|                  null|             null|This is comment t...|               0| 1134385225|         null| false|     0|c27|t3_17844| t3_17844|  1473738413|  -16|   false|reddit.com|        t5_6|-16|\n",
      "| [deleted]|                  null|             null|           [deleted]|               1| 1134386131|         null| false|     0|c29|t3_17844| t3_17844|  1473738413|    0|   false|reddit.com|        t5_6|  0|\n",
      "| [deleted]|                  null|             null|           [deleted]|               1| 1134386182|         null| false|     0|c30|t3_17844| t3_17844|  1473738413|   -2|   false|reddit.com|        t5_6| -2|\n",
      "| [deleted]|                  null|             null|           [deleted]|               1| 1134386193|         null| false|     0|c31|t3_17844| t3_17844|  1473738413|   -6|   false|reddit.com|        t5_6| -6|\n",
      "| [deleted]|                  null|             null|           [deleted]|               1| 1134386254|         null| false|     0|c32|t3_17844| t3_17844|  1473738413|   -4|   false|reddit.com|        t5_6| -4|\n",
      "|   AaronSw|                  null|             null|It's a New York T...|               0| 1134386388|         null| false|     0|c33|t3_17844|   t1_c29|  1473738413|    3|   false|reddit.com|        t5_6|  3|\n",
      "|   AaronSw|                  null|             null|[Here's the copy ...|               0| 1134386498|         null| false|     0|c35|t3_17844|   t1_c33|  1473738413|   12|   false|reddit.com|        t5_6| 12|\n",
      "|  fnord123|                  null|             null|The best thing ab...|               1| 1134386557|         null| false|     0|c36|t3_17844| t3_17844|  1473738413|    5|   false|reddit.com|        t5_6|  5|\n",
      "+----------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+---+--------+---------+------------+-----+--------+----------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: boolean (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_contro=df1.select( 'ups','score', 'controversiality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"ups\", \"score\"], outputCol=\"features\")\n",
    "labelIndexer = StringIndexer(inputCol='controversiality', outputCol=\"label\")\n",
    "stg=[assembler, labelIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_cdf, test_cdf)=preprocessed_contro.randomSplit([0.8 ,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol= \"features\", labelCol=\"label\",  maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl=Pipeline(stages= [assembler, labelIndexer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ppl.fit(train_cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cdf = model.transform(test_cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------------+-------------+-----+--------------------+--------------------+----------+\n",
      "|ups|score|controversiality|     features|label|       rawPrediction|         probability|prediction|\n",
      "+---+-----+----------------+-------------+-----+--------------------+--------------------+----------+\n",
      "|-16|  -16|               0|[-16.0,-16.0]|  0.0|[0.50416186186452...|[0.62343688455997...|       0.0|\n",
      "| -9|   -9|               0|  [-9.0,-9.0]|  0.0|[1.56039474723286...|[0.82640998934167...|       0.0|\n",
      "| -6|   -6|               0|  [-6.0,-6.0]|  0.0|[2.01306598381929...|[0.88216211133835...|       0.0|\n",
      "| -6|   -6|               1|  [-6.0,-6.0]|  1.0|[2.01306598381929...|[0.88216211133835...|       0.0|\n",
      "| -5|   -5|               0|  [-5.0,-5.0]|  0.0|[2.16395639601477...|[0.89696576614953...|       0.0|\n",
      "| -4|   -4|               0|  [-4.0,-4.0]|  0.0|[2.31484680821024...|[0.91009920358905...|       0.0|\n",
      "| -4|   -4|               0|  [-4.0,-4.0]|  0.0|[2.31484680821024...|[0.91009920358905...|       0.0|\n",
      "| -4|   -4|               0|  [-4.0,-4.0]|  0.0|[2.31484680821024...|[0.91009920358905...|       0.0|\n",
      "| -4|   -4|               0|  [-4.0,-4.0]|  0.0|[2.31484680821024...|[0.91009920358905...|       0.0|\n",
      "| -3|   -3|               0|  [-3.0,-3.0]|  0.0|[2.46573722040572...|[0.92170469320192...|       0.0|\n",
      "| -2|   -2|               0|  [-2.0,-2.0]|  0.0|[2.61662763260120...|[0.93192406943289...|       0.0|\n",
      "| -2|   -2|               0|  [-2.0,-2.0]|  0.0|[2.61662763260120...|[0.93192406943289...|       0.0|\n",
      "| -2|   -2|               0|  [-2.0,-2.0]|  0.0|[2.61662763260120...|[0.93192406943289...|       0.0|\n",
      "| -2|   -2|               0|  [-2.0,-2.0]|  0.0|[2.61662763260120...|[0.93192406943289...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "| -1|   -1|               0|  [-1.0,-1.0]|  0.0|[2.76751804479667...|[0.94089511222752...|       0.0|\n",
      "+---+-----+----------------+-------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 94.6602 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction_cdf)\n",
    "print(\"Accuracy = %g \" % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(df):\n",
    "    preprocessed_contro=df.select( 'ups','score', 'controversiality')\n",
    "    assembler = VectorAssembler(inputCols=[\"ups\", \"score\"], outputCol=\"features\")\n",
    "    labelIndexer = StringIndexer(inputCol='controversiality', outputCol=\"label\")\n",
    "    lr = LogisticRegression(featuresCol= \"features\", labelCol=\"label\",  maxIter=10)\n",
    "    ppl=Pipeline(stages= [assembler, labelIndexer, lr])\n",
    "    (train_cdf, test_cdf)=preprocessed_contro.randomSplit([0.8 ,0.2])\n",
    "    model=ppl.fit(train_cdf)\n",
    "    # Select (prediction, true label) and compute test accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(prediction_cdf)\n",
    "    print(\"Accuracy = %g \" % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = df1.select('body', 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|                body|score|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|A look at Vietnam...|    2|[a, look, at, vie...|\n",
      "|The site states \"...|    1|[the, site, state...|\n",
      "|Jython related to...|    0|[jython, related,...|\n",
      "|           [deleted]|    1|         [[deleted]]|\n",
      "|Saft is by far th...|    1|[saft, is, by, fa...|\n",
      "|           [deleted]|   -6|         [[deleted]]|\n",
      "|How to take panor...|    1|[how, to, take, p...|\n",
      "|I donât know wh...|    1|[i, donât, know...|\n",
      "|LinkIt by Marc, a...|    1|[linkit, by, marc...|\n",
      "|Making websites r...|    1|[making, websites...|\n",
      "|On the bright sid...|    1|[on, the, bright,...|\n",
      "|Like a lot of peo...|    8|[like, a, lot, of...|\n",
      "|This is comment t...|  -16|[this, is, commen...|\n",
      "|           [deleted]|    0|         [[deleted]]|\n",
      "|           [deleted]|   -2|         [[deleted]]|\n",
      "|           [deleted]|   -6|         [[deleted]]|\n",
      "|           [deleted]|   -4|         [[deleted]]|\n",
      "|It's a New York T...|    3|[it's, a, new, yo...|\n",
      "|[Here's the copy ...|   12|[[here's, the, co...|\n",
      "|The best thing ab...|    5|[the, best, thing...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformer 1: Tokenizer (splits up words)\n",
    "tokenizer = Tokenizer(inputCol=\"body\", outputCol=\"words\")\n",
    "tokenized_df = tokenizer.transform(preprocessed_df)\n",
    "tokenized_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(body='A look at Vietnam and Mexico exposes the myth of market liberalisation.', score=2, words=['a', 'look', 'at', 'vietnam', 'and', 'mexico', 'exposes', 'the', 'myth', 'of', 'market', 'liberalisation.'], features=SparseVector(262144, {2624: 1.0, 9639: 1.0, 91677: 1.0, 103838: 1.0, 114232: 1.0, 135373: 1.0, 138730: 1.0, 170698: 1.0, 176964: 1.0, 223763: 1.0, 227410: 1.0, 237314: 1.0})),\n",
       " Row(body='The site states \"What can I use it for? Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more...\", just like any other new breeed of sites that want us to store everything we have on the web. And they even guarantee multiple levels of security and encryption etc. But what prevents these web site operators fom accessing and/or stealing Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more, for competitive or personal gains...? I am pretty sure that most of them are honest, but what\\'s there to prevent me from setting up a good useful site and stealing all your data? Call me paranoid - I am.', score=1, words=['the', 'site', 'states', '\"what', 'can', 'i', 'use', 'it', 'for?', 'meeting', 'notes,', 'reports,', 'technical', 'specs', 'sign-up', 'sheets,', 'proposals', 'and', 'much', 'more...\",', 'just', 'like', 'any', 'other', 'new', 'breeed', 'of', 'sites', 'that', 'want', 'us', 'to', 'store', 'everything', 'we', 'have', 'on', 'the', 'web.', 'and', 'they', 'even', 'guarantee', 'multiple', 'levels', 'of', 'security', 'and', 'encryption', 'etc.', 'but', 'what', 'prevents', 'these', 'web', 'site', 'operators', 'fom', 'accessing', 'and/or', 'stealing', 'meeting', 'notes,', 'reports,', 'technical', 'specs', 'sign-up', 'sheets,', 'proposals', 'and', 'much', 'more,', 'for', 'competitive', 'or', 'personal', 'gains...?', 'i', 'am', 'pretty', 'sure', 'that', 'most', 'of', 'them', 'are', 'honest,', 'but', \"what's\", 'there', 'to', 'prevent', 'me', 'from', 'setting', 'up', 'a', 'good', 'useful', 'site', 'and', 'stealing', 'all', 'your', 'data?', 'call', 'me', 'paranoid', '-', 'i', 'am.'], features=SparseVector(262144, {2362: 1.0, 2751: 1.0, 7367: 1.0, 9639: 3.0, 16332: 1.0, 17132: 1.0, 20633: 2.0, 20743: 1.0, 21140: 1.0, 21872: 1.0, 24112: 1.0, 24417: 3.0, 24847: 2.0, 29945: 1.0, 32296: 2.0, 32764: 1.0, 34116: 1.0, 36073: 1.0, 42003: 1.0, 45531: 1.0, 48448: 2.0, 51452: 1.0, 52206: 2.0, 55039: 1.0, 58971: 2.0, 60697: 1.0, 60905: 1.0, 76764: 2.0, 79737: 1.0, 79793: 1.0, 80003: 1.0, 81566: 1.0, 86175: 1.0, 88637: 1.0, 91451: 2.0, 91677: 5.0, 91862: 1.0, 94851: 3.0, 97171: 1.0, 98022: 1.0, 99346: 1.0, 100258: 1.0, 101169: 1.0, 103838: 2.0, 108647: 1.0, 109706: 1.0, 113432: 1.0, 115587: 1.0, 116873: 1.0, 134125: 1.0, 135499: 1.0, 135560: 1.0, 147489: 1.0, 149173: 1.0, 151536: 1.0, 166629: 1.0, 166690: 2.0, 167122: 1.0, 167131: 1.0, 174966: 1.0, 175449: 1.0, 176968: 1.0, 179344: 1.0, 179459: 1.0, 180535: 1.0, 186048: 2.0, 189082: 1.0, 189683: 2.0, 190256: 1.0, 193098: 1.0, 193284: 1.0, 194186: 1.0, 199620: 1.0, 200912: 1.0, 205044: 2.0, 208258: 1.0, 218334: 1.0, 221047: 2.0, 225103: 1.0, 227410: 1.0, 232467: 1.0, 236565: 1.0, 247107: 1.0, 253475: 1.0, 259248: 1.0, 261415: 2.0})),\n",
       " Row(body='Jython related topics by Frank Wierzbicki', score=0, words=['jython', 'related', 'topics', 'by', 'frank', 'wierzbicki'], features=SparseVector(262144, {15207: 1.0, 30425: 1.0, 54370: 1.0, 104570: 1.0, 133143: 1.0, 176140: 1.0}))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer 2: Convert Words into word frequencies (TF = \"Term Frequency\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "freq_df = hashingTF.transform(tokenized_df)\n",
    "freq_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: Random Forest Regression\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"score\")\n",
    "\n",
    "# Put them together as a pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_rdf, test_rdf)=preprocessed_df.randomSplit([0.8 ,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                body|score|\n",
      "+--------------------+-----+\n",
      "|A look at Vietnam...|    2|\n",
      "|The site states \"...|    1|\n",
      "|Jython related to...|    0|\n",
      "|           [deleted]|    1|\n",
      "|Saft is by far th...|    1|\n",
      "|           [deleted]|   -6|\n",
      "|How to take panor...|    1|\n",
      "|I donât know wh...|    1|\n",
      "|LinkIt by Marc, a...|    1|\n",
      "|Making websites r...|    1|\n",
      "|On the bright sid...|    1|\n",
      "|Like a lot of peo...|    8|\n",
      "|This is comment t...|  -16|\n",
      "|           [deleted]|    0|\n",
      "|           [deleted]|   -2|\n",
      "|           [deleted]|   -6|\n",
      "|           [deleted]|   -4|\n",
      "|It's a New York T...|    3|\n",
      "|[Here's the copy ...|   12|\n",
      "|The best thing ab...|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o819.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 87 (collectAsMap at RandomForest.scala:928) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 19 \tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697) \tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693) \tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) \tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) \tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) \tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) \tat org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693) \tat org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:108) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1310)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:746)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:745)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:928)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:906)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:118)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:111)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-ca7157413941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodelreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \"\"\"\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o819.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 87 (collectAsMap at RandomForest.scala:928) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 19 \tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:697) \tat org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:693) \tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) \tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) \tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) \tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) \tat org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:693) \tat org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:147) \tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) \tat org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) \tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323) \tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:108) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748) \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1310)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1711)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:746)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:745)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplitsBySorting(RandomForest.scala:928)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findSplits(RandomForest.scala:906)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:118)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:111)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "modelreg=pipeline.fit(train_rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rdf = model.transform(test_rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
