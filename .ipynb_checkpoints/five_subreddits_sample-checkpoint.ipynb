{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark libraries and time (for execution calculation)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import time\n",
    "\n",
    "# Broadcasttimeout is increased to accomodate and not stop the process,\n",
    "# when the data is huge and number of cores given are less\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://192.168.2.75:7077\")\\\n",
    "            .appName(\"Test_reddit_sample\")\\\n",
    "            .config(\"spark.executor.cores\",4)\\\n",
    "            .config(\"spark.sql.broadcastTimeout\", 36000)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required dataset\n",
    "io_start = time.time()\n",
    "df1 =  spark_session.read.json('hdfs://192.168.2.75:9000/data/reddit_sample.json')\n",
    "\n",
    "# Storing the time taken for loading the data as dataframe\n",
    "io_time = time.time() - io_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(subreddit='AskReddit', count=486), Row(subreddit='CFB', count=403), Row(subreddit='CrazyIdeas', count=261), Row(subreddit='news', count=158), Row(subreddit='ConciseIAmA', count=147)]\n"
     ]
    }
   ],
   "source": [
    "# Start time of code execution\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Finding the subreddit with maximum author count and taking 5 of them for further processing\n",
    "\n",
    "df5 = df1.select(\"author\",\"subreddit\")\\\n",
    "        .groupBy(\"subreddit\")\\\n",
    "        .count()\\\n",
    "        .orderBy([\"count\"],ascending=False)\\\n",
    "        .limit(5)\n",
    "\n",
    "print(df5.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('AskReddit', 'the'), 435), (('AskReddit', 'a'), 333), (('AskReddit', 'to'), 306), (('AskReddit', 'and'), 300), (('CrazyIdeas', 'this'), 259), (('CrazyIdeas', 'lets'), 258), (('CrazyIdeas', 'see'), 258), (('CrazyIdeas', 'how'), 258), (('CrazyIdeas', 'deep'), 258), (('CrazyIdeas', 'rabbit'), 258)]\n"
     ]
    }
   ],
   "source": [
    "# importing regular expression library\n",
    "\n",
    "from re import sub\n",
    "\n",
    "# Converting string to lower case and remove extra characters from the string\n",
    "# then splitting the string to list of words with each word having count \"1\"\n",
    "\n",
    "def map_values(subreddit,string):\n",
    "    string_out = sub(r'[^A-Za-z0-9 \\'/]+', u'', string.lower())\n",
    "    string_out = string_out.split(' ')\n",
    "    entry_list = []\n",
    "    for entry in string_out:\n",
    "        entry_list.append([(subreddit,entry),1])\n",
    "    return (entry_list)\n",
    "\n",
    "# Joining the df5 with five subreddits and the whole dataset to get \n",
    "# only data for five subreddit and extracting only the body of the subreddit \n",
    "# and converting it into RDD for map reduce\n",
    "# Using map_values mapping the words, and reducing as a key pair with key as\n",
    "# (subreddit,word) and sorting the reduced value\n",
    "\n",
    "rdd1 = df5.join(df1.select(\"subreddit\",\"body\"),df5.subreddit==df1.subreddit)\\\n",
    "        .select(df1.subreddit,\"body\").rdd\\\n",
    "        .flatMap(lambda x: map_values(x[0],x[1]))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .sortBy(lambda x:x[1],ascending=False)\n",
    "\n",
    "# Calculating the execution time of the code and storing it\n",
    "stop_time = time.time()\n",
    "time_execution = stop_time - start_time\n",
    "\n",
    "print(rdd1.take(10),time_execution,io_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('CrazyIdeas', 'this'), 259), (('CrazyIdeas', 'lets'), 258), (('CrazyIdeas', 'see'), 258), (('CrazyIdeas', 'how'), 258), (('CrazyIdeas', 'deep'), 258), (('CrazyIdeas', 'rabbit'), 258), (('CrazyIdeas', 'hole'), 258), (('CrazyIdeas', 'goes'), 258), (('CrazyIdeas', 'i'), 3), (('CrazyIdeas', 'never'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Example output of word count from the subreddit \"CrazyIdeas\"\n",
    "\n",
    "CrazyIdeas = rdd1.filter(lambda x: x[0][0]=='CrazyIdeas')\n",
    "\n",
    "print(CrazyIdeas.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
